{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 180 Group 4: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the Housing and Water Expenditure Based on Household-Level Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project aims to develop a regression model that can predict housing and water expenditure based on user information. The group's objective is to assist Filipino households in their financial management and provide future household heads with estimates of costs related to housing, water, and electricity, thereby aiding them in their family or house plans."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv('Family Income and Expenditure.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the data structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### \n",
    "Generate Pandas Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling as pp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Uncomment the lines below to generate a new profile report.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report = pp.ProfileReport(df_dataset)\n",
    "# report.to_file('profile_report.html')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the detailed profile report, with the filename of 'profile_report.html', was pre-generated using the same code above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### \n",
    "Examine structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### \n",
    "Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = df_dataset.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.9, square=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### \n",
    "Correlation with House and Water Expenditure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Housing and water Expenditure'\n",
    "k = 12\n",
    "corrmat = df_dataset.corr()\n",
    "cols = corrmat.nlargest(k, target)[target].index\n",
    "f, ax = plt.subplots(figsize=(10, 7))\n",
    "cm = np.corrcoef(df_dataset[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "\n",
    "\n",
    "s = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "s.set_title(\"Top 12 Variables Most Correlated with House and water Expenditure\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the project by importing the necessary libraries and modules. Additionally, we load the FIES (Filipino Income and Expenditure) dataset and clean it by utilizing only the initially identified features and dropping rows with null values.\n",
    "\n",
    "Dataset source: https://www.kaggle.com/datasets/grosvenpaul/family-income-and-expenditure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we merge the features 'Members with age less than 5 years old' and 'Members with age 5 - 17 years old' to create the 'Number of Children' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# Load the dataset\n",
    "df_dataset = pd.read_csv('Family Income and Expenditure.csv')\n",
    "\n",
    "# Initial features to use\n",
    "features = [\n",
    "    \"Housing and water Expenditure\",\n",
    "    \"Total Household Income\",\n",
    "    \"Region\",\n",
    "    \"Agricultural Household indicator\",\n",
    "    \"Imputed House Rental Value\",\n",
    "    \"Total Income from Entrepreneurial Acitivites\",\n",
    "    \"Total Number of Family members\",\n",
    "    \"Members with age less than 5 year old\",\n",
    "    \"Members with age 5 - 17 years old\",\n",
    "    \"Total number of family members employed\",\n",
    "    \"Type of Building/House\",\n",
    "    \"Type of Roof\",\n",
    "    \"Type of Walls\",\n",
    "    \"House Floor Area\",\n",
    "    \"House Age\",\n",
    "    \"Number of bedrooms\",\n",
    "    \"Electricity\",\n",
    "    \"Main Source of Water Supply\",\n",
    "    \"Number of Television\",\n",
    "    \"Number of CD/VCD/DVD\",\n",
    "    \"Number of Component/Stereo set\",\n",
    "    \"Number of Refrigerator/Freezer\",\n",
    "    \"Number of Washing Machine\",\n",
    "    \"Number of Airconditioner\",\n",
    "    \"Number of Personal Computer\"\n",
    "]\n",
    "\n",
    "# Drop features that are not in the initial set of features\n",
    "df = df_dataset.filter(features, axis=1)\n",
    "\n",
    "# To clean the dataset:\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Create feature called \"Number of Children\"\n",
    "df['Number of Children'] = df['Members with age less than 5 year old'] + df['Members with age 5 - 17 years old']\n",
    "\n",
    "# Drop combined features\n",
    "df = df.drop(['Members with age less than 5 year old', 'Members with age 5 - 17 years old'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will identify the list of unique categories for three categorical features: Region, Type of Building/House, and Main Source of Water Supply. Note that this step is necessary since, as we will see later, these three features are included in the final Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify unique values for each of the three categorical feature included in the final regression model\n",
    "\n",
    "# list region names\n",
    "region_names = list(df['Region'].unique())\n",
    "print(f'Regions: {region_names}')\n",
    "print()\n",
    "# output: ['CAR', 'Caraga', 'VI - Western Visayas', 'V - Bicol Region', ' ARMM', 'III - Central Luzon', 'II - Cagayan Valley', 'IVA - CALABARZON', 'VII - Central Visayas', 'X - Northern Mindanao', 'XI - Davao Region', 'VIII - Eastern Visayas', 'I - Ilocos Region', 'NCR', 'IVB - MIMAROPA', 'XII - SOCCSKSARGEN', 'IX - Zasmboanga Peninsula']\n",
    "\n",
    "# list type of building/house\n",
    "house_types = list(df['Type of Building/House'].unique())\n",
    "print(f'Type of Building/House: {house_types}')\n",
    "print()\n",
    "# output : ['Single house', 'Duplex', 'Commercial/industrial/agricultural building', 'Multi-unit residential', 'Institutional living quarter', 'Other building unit (e.g. cave, boat)']\n",
    "\n",
    "# list main source of water supply\n",
    "water_sources = list(df['Main Source of Water Supply'].unique())\n",
    "print(f'Main Source of Water Supply: {water_sources}') \n",
    "print()\n",
    "# output : ['Own use, faucet, community water system', 'Shared, faucet, community water system', 'Shared, tubed/piped deep well', 'Own use, tubed/piped deep well', 'Protected spring, river, stream, etc', 'Tubed/piped shallow well', 'Lake, river, rain and others', 'Unprotected spring, river, stream, etc', 'Dug well', 'Others', 'Peddler']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspection, we can see two typographical errors. These errors are the ' ARMM' and 'IX - Zasmboanga Peninsula' categories of the Region feature. We will correct them using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct typos in region names\n",
    "df['Region'] = df['Region'].replace(' ARMM', 'ARMM')\n",
    "df['Region'] = df['Region'].replace('IX - Zasmboanga Peninsula', 'IX - Zamboanga Peninsula')\n",
    "\n",
    "# Update region names\n",
    "region_names = list(df['Region'].unique())\n",
    "print(region_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the typographical errors have now been collected.\n",
    "\n",
    "Since we have categorical variables, we use target encoding to convert categorical values into numerical ones. We also store the encoding map for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical columns\n",
    "cat_cols = ['Region', 'Type of Building/House', 'Type of Roof', 'Type of Walls', 'Main Source of Water Supply']\n",
    "\n",
    "# Create an instance of the TargetEncoder from the category_encoders library\n",
    "# The purpose of the TargetEncoder is to transform categorical features into numeric representations\n",
    "encoder = ce.TargetEncoder(cols=cat_cols)\n",
    "\n",
    "# Fit and transform the target encoder on the dataframe\n",
    "df_encoded = encoder.fit_transform(df, df['Housing and water Expenditure'])\n",
    "\n",
    "# Access the mapping\n",
    "mapping = encoder.mapping\n",
    "\n",
    "# Replace the original categorical columns with the target-encoded values\n",
    "df[cat_cols] = df_encoded[cat_cols]\n",
    "\n",
    "# Create a dictionary mapping the categorical columns to the target encoding\n",
    "cat_dict_map = {}\n",
    "for column in cat_cols:\n",
    "    cat_dict_map[column] = list(mapping[column])[:-2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the encoding map, we create dictionaries for converting the categorical features included in the final regression model. To avoid confusion, we note that these features are identified by the feature selection method, which will be performed later on. Essentially, we added these codes to this part because we already know that they will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping the categorical columns to the target encoding\n",
    "\n",
    "convert_region = {k: v for k, v in zip(region_names, cat_dict_map['Region'])}\n",
    "convert_house_type = {k: v for k, v in zip(house_types, cat_dict_map['Type of Building/House'])}\n",
    "convert_water_source = {k: v for k, v in zip(water_sources, cat_dict_map['Main Source of Water Supply'])}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to begin building our regression model. We start by separating the features from the target, which is 'Housing and Water Expenditure'. Then, we determine highly correlated features and drop them. However, we can see that the initial features are not highly correlated. Thus, no features are dropped at this stage.\n",
    "\n",
    "Link to source code:\n",
    "https://stackoverflow.com/questions/29294983/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into X and y\n",
    "X = df.drop(['Housing and water Expenditure'], axis=1)\n",
    "y = df['Housing and water Expenditure']\n",
    "\n",
    "# Analyze the correlation among the features\n",
    "threshold = 0.9\n",
    "corr_matrix = X.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
    "upper.head()\n",
    "\n",
    "# Drop columns with correlations above threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "if len(to_drop) > 0:\n",
    "    print(f'Columns to drop: {to_drop}')\n",
    "    X = X.drop(to_drop, axis=1)\n",
    "else:\n",
    "    print('No columns to drop. The features are not highly correlated.')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Training and Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we determine the best features to use in the regression model. We split our data for training and testing. Additionally, we observe that a linear regression model was used for sequential feature selection. This is because it is the simplest regression model, and feature selection requires a significant amount of computation power and time. We drop the features that are not selected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we will be using the class sklearn.feature_selection.SequentialFeatureSelector from the scikit-learn library that implements a sequential feature selection algorithm. This module adds or removes (in our case, we will be looking at removing features)  to form a feature subset in a greedy fashion. At each stage, this estimator chooses the best feature to add or remove based on the cross-validation score of an estimator.\n",
    "\n",
    "More about this here:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n",
    "\n",
    "# Forward feature selection\n",
    "regressor = LinearRegression()\n",
    "sfs = SequentialFeatureSelector(regressor, k_features='best', forward=True, scoring='neg_mean_squared_error', cv=5)\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "selected_features = list(X_train.columns[list(sfs.k_feature_idx_)])\n",
    "print(f'Selected Features: {selected_features}')\n",
    "\n",
    "# Drop features that are not selected\n",
    "to_drop = [feature for feature in X.columns if feature not in selected_features]\n",
    "X = X.drop(to_drop, axis=1)\n",
    "print(f'Dropped features: {to_drop}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. Choosing Machine Learning Model To Use"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we decided to use a Linear Regression Model since it was discussed and presented clearly by our course adviser. However, we decided to test the Linear Regression Model against other models and compare their metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "STOPPING EARLY DUE TO KEYBOARD INTERRUPT..."
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m sfs \u001b[39m=\u001b[39m SequentialFeatureSelector(regressor, k_features\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbest\u001b[39m\u001b[39m'\u001b[39m, forward\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mneg_mean_squared_error\u001b[39m\u001b[39m'\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m      7\u001b[0m sfs\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m----> 9\u001b[0m selected_features \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(X_train\u001b[39m.\u001b[39mcolumns[\u001b[39mlist\u001b[39;49m(sfs\u001b[39m.\u001b[39;49mk_feature_idx_)])\n\u001b[0;32m     11\u001b[0m \u001b[39m# Drop features that are not selected\u001b[39;00m\n\u001b[0;32m     12\u001b[0m to_drop \u001b[39m=\u001b[39m [feature \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m X\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m feature \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m selected_features]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test_linear = train_test_split(X, y, test_size=0.30, random_state=1)\n",
    "\n",
    "# Forward feature selection\n",
    "regressor = LinearRegression()\n",
    "sfs = SequentialFeatureSelector(regressor, k_features='best', forward=True, scoring='neg_mean_squared_error', cv=5)\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "selected_features = list(X_train.columns[list(sfs.k_feature_idx_)])\n",
    "\n",
    "# Drop features that are not selected\n",
    "to_drop = [feature for feature in X.columns if feature not in selected_features]\n",
    "X = X.drop(to_drop, axis=1)\n",
    "\n",
    "# Fit the model and predict the target\n",
    "regressor.fit(X_train, y_train)\n",
    "linear_pred = regressor.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "MSE_linear = mean_squared_error(y_test_linear, linear_pred)\n",
    "RMSE_linear = np.sqrt(mean_squared_error(y_test_linear, linear_pred))\n",
    "MAE_linear = mean_absolute_error(y_test_linear, linear_pred)\n",
    "R2_linear = r2_score(y_test_linear, linear_pred)\n",
    "\n",
    "# Show and plot the metrics and the results\n",
    "print(f'MSE: {MSE_linear}')\n",
    "print(f'RMSE: {RMSE_linear}')\n",
    "print(f'MAE: {MAE_linear}')\n",
    "print(f'R2: {R2_linear}')\n",
    "\n",
    "plt.scatter(y_test_linear, linear_pred)\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Predicted vs Target')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test_ridge = train_test_split(X, y, test_size=0.30, random_state=1)\n",
    "\n",
    "# Define the parameter grid for alpha values\n",
    "param_grid = {'alpha': np.logspace(-4, 2, 50)}\n",
    "\n",
    "ridge_model = Ridge()\n",
    "grid_search = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best alpha value\n",
    "print(\"Best alpha: \", grid_search.best_params_['alpha'])\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "ridge_best = Ridge(alpha=grid_search.best_params_['alpha'])\n",
    "ridge_best.fit(X_train, y_train)\n",
    "ridge_pred = ridge_best.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "MSE_ridge = mean_squared_error(y_test_ridge, ridge_pred)\n",
    "RMSE_ridge = np.sqrt(mean_squared_error(y_test_ridge, ridge_pred))\n",
    "MAE_ridge = mean_absolute_error(y_test_ridge, ridge_pred)\n",
    "R2_ridge = r2_score(y_test_ridge, ridge_pred)\n",
    "\n",
    "# Show and plot the metrics and the results\n",
    "print(f'MSE: {MSE_ridge}')\n",
    "print(f'RMSE: {RMSE_ridge}')\n",
    "print(f'MAE: {MAE_ridge}')\n",
    "print(f'R2: {R2_ridge}')\n",
    "\n",
    "plt.scatter(y_test_ridge, ridge_pred)\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Predicted vs Target')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test_lasso = train_test_split(X, y, test_size=0.30, random_state=1)\n",
    "\n",
    "# Define the parameter grid for alpha values\n",
    "param_grid = {'alpha': np.logspace(-4, 2, 50)}\n",
    "\n",
    "lasso_model = Lasso()\n",
    "grid_search = GridSearchCV(lasso_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best alpha value\n",
    "print(\"Best alpha: \", grid_search.best_params_['alpha'])\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "lasso_best = Lasso(alpha=grid_search.best_params_['alpha'])\n",
    "lasso_best.fit(X_train, y_train)\n",
    "lasso_pred = lasso_best.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "MSE_lasso = mean_squared_error(y_test_lasso, lasso_pred)\n",
    "RMSE_lasso = np.sqrt(mean_squared_error(y_test_lasso, lasso_pred))\n",
    "MAE_lasso = mean_absolute_error(y_test_lasso, lasso_pred)\n",
    "R2_lasso = r2_score(y_test_lasso, lasso_pred)\n",
    "\n",
    "# Show and plot the metrics and the results\n",
    "print(f'MSE: {MSE_lasso}')\n",
    "print(f'RMSE: {RMSE_lasso}')\n",
    "print(f'MAE: {MAE_lasso}')\n",
    "print(f'R2: {R2_lasso}')\n",
    "\n",
    "plt.scatter(y_test_lasso, lasso_pred)\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Predicted vs Target')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Account for the interaction between features using a polynomial of degree 2\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_interactions = poly.fit_transform(X)\n",
    "\n",
    "# Split the data into train and test sets using X_interactions instead of X\n",
    "X_train, X_test, y_train, y_test_poly = train_test_split(X_interactions, y, test_size=0.30, random_state=1)\n",
    "\n",
    "# Fit the model and predict the target\n",
    "regressor.fit(X_train, y_train)\n",
    "poly_pred = regressor.predict(X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "MSE_poly = mean_squared_error(y_test_poly, poly_pred)\n",
    "RMSE_poly = np.sqrt(mean_squared_error(y_test_poly, poly_pred))\n",
    "MAE_poly = mean_absolute_error(y_test_poly, poly_pred)\n",
    "R2_poly = r2_score(y_test_poly, poly_pred)\n",
    "\n",
    "# Show and plot the metrics and the results\n",
    "print(f'MSE: {MSE_poly}')\n",
    "print(f'RMSE: {RMSE_poly}')\n",
    "print(f'MAE: {MAE_poly}')\n",
    "print(f'R2: {R2_poly}')\n",
    "\n",
    "plt.scatter(y_test_poly, poly_pred)\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Predicted vs Target')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural Network Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Neural Network model uses all of the columns as opposed to selected features to take advantage of the properties of neural networks and the amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "import matplotlib.pyplot as plt\n",
    "import category_encoders as ce\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Load the dataset\n",
    "df_dataset = pd.read_csv('Family Income and Expenditure.csv')\n",
    "\n",
    "# Identify the list of unique categories for the categorical features\n",
    "region = list(df_dataset['Region'].unique())\n",
    "main_source_of_income = list(df_dataset['Main Source of Income'].unique())\n",
    "household_head_sex = list(df_dataset['Household Head Sex'].unique())\n",
    "household_head_marital_status = list(df_dataset['Household Head Marital Status'].unique())\n",
    "household_head_highest_grade_completed = list(df_dataset['Household Head Highest Grade Completed'].unique())\n",
    "household_head_job_or_business_indicator = list(df_dataset['Household Head Job or Business Indicator'].unique())\n",
    "household_head_occupation = list(df_dataset['Household Head Occupation'].unique())\n",
    "household_head_class_of_worker = list(df_dataset['Household Head Class of Worker'].unique())\n",
    "type_of_household = list(df_dataset['Type of Household'].unique())\n",
    "type_of_buildingHouse = list(df_dataset['Type of Building/House'].unique())\n",
    "type_of_roof = list(df_dataset['Type of Roof'].unique())\n",
    "type_of_walls = list(df_dataset['Type of Walls'].unique())\n",
    "tenure_status = list(df_dataset['Tenure Status'].unique())\n",
    "toilet_facilities = list(df_dataset['Toilet Facilities'].unique())\n",
    "main_source_of_water_supply = list(df_dataset['Main Source of Water Supply'].unique())\n",
    "\n",
    "# correct typos in region names\n",
    "df_dataset['Region'] = df_dataset['Region'].replace(' ARMM', 'ARMM')\n",
    "df_dataset['Region'] = df_dataset['Region'].replace('IX - Zasmboanga Peninsula', 'IX - Zamboanga Peninsula')\n",
    "\n",
    "# update region names\n",
    "region = list(df_dataset['Region'].unique())\n",
    "\n",
    "# Define the categorical columns\n",
    "cat_cols = ['Region', 'Main Source of Income', 'Household Head Sex', 'Household Head Marital Status', 'Household Head Highest Grade Completed', 'Household Head Job or Business Indicator', 'Household Head Occupation', 'Household Head Class of Worker', 'Type of Household', 'Type of Building/House', 'Type of Roof', 'Type of Walls', 'Tenure Status', 'Toilet Facilities', 'Main Source of Water Supply', ]\n",
    "\n",
    "# Create an instance of the TargetEncoder\n",
    "encoder = ce.TargetEncoder(cols=cat_cols)\n",
    "\n",
    "# Fit and transform the target encoder on the dataframe\n",
    "df_encoded = encoder.fit_transform(df_dataset, df_dataset['Housing and water Expenditure'])\n",
    "\n",
    "# Access the mapping\n",
    "mapping = encoder.mapping\n",
    "\n",
    "# Replace the original categorical columns with the target-encoded values\n",
    "df_dataset[cat_cols] = df_encoded[cat_cols]\n",
    "\n",
    "# Create a dictionary mapping the categorical columns to the target encoding\n",
    "cat_dict_map = {}\n",
    "for column in cat_cols:\n",
    "    cat_dict_map[column] = list(mapping[column])[:-2]\n",
    "\n",
    "convert_region = {k: v for k, v in zip(region, cat_dict_map['Region'])}\n",
    "convert_main_source_of_income = {k: v for k, v in zip(main_source_of_income, cat_dict_map['Main Source of Income'])}\n",
    "convert_household_head_sex = {k: v for k, v in zip(household_head_sex, cat_dict_map['Household Head Sex'])}\n",
    "convert_household_head_marital_status = {k: v for k, v in zip(household_head_marital_status, cat_dict_map['Household Head Marital Status'])}\n",
    "convert_household_head_highest_grade_completed = {k: v for k, v in zip(household_head_highest_grade_completed, cat_dict_map['Household Head Highest Grade Completed'])}\n",
    "convert_household_head_job_or_business_indicator = {k: v for k, v in zip(household_head_job_or_business_indicator, cat_dict_map['Household Head Job or Business Indicator'])}\n",
    "convert_household_head_occupation = {k: v for k, v in zip(household_head_occupation, cat_dict_map['Household Head Occupation'])}\n",
    "convert_household_head_class_of_worker = {k: v for k, v in zip(household_head_class_of_worker, cat_dict_map['Household Head Class of Worker'])}\n",
    "convert_type_of_household = {k: v for k, v in zip(type_of_household, cat_dict_map['Type of Household'])}\n",
    "convert_type_of_buildingHouse = {k: v for k, v in zip(type_of_buildingHouse, cat_dict_map['Type of Building/House'])}\n",
    "convert_type_of_roof = {k: v for k, v in zip(type_of_roof, cat_dict_map['Type of Roof'])}\n",
    "convert_type_of_walls = {k: v for k, v in zip(type_of_walls, cat_dict_map['Type of Walls'])}\n",
    "convert_tenure_status = {k: v for k, v in zip(tenure_status, cat_dict_map['Tenure Status'])}\n",
    "convert_toilet_facilities = {k: v for k, v in zip(toilet_facilities, cat_dict_map['Toilet Facilities'])}\n",
    "convert_main_source_of_water_supply = {k: v for k, v in zip(main_source_of_water_supply, cat_dict_map['Main Source of Water Supply'])}\n",
    "\n",
    "# columns = ['Housing and water Expenditure', 'Total Household Income', 'Region', 'Imputed House Rental Value', 'Total Number of Family members', 'Type of Building/House', 'House Floor Area', 'House Age', 'Number of bedrooms', 'Main Source of Water Supply', 'Number of CD/VCD/DVD', 'Number of Component/Stereo set', 'Number of Refrigerator/Freezer', 'Number of Washing Machine', 'Number of Airconditioner', 'Number of Personal Computer']\n",
    "# df_dataset = df_dataset.filter(columns, axis=1)\n",
    "\n",
    "# Split into input (X) and output (y) variables\n",
    "X = df_dataset.drop(['Housing and water Expenditure'], axis=1)\n",
    "y = df_dataset['Housing and water Expenditure']\n",
    "\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, y, test_size = 0.3, random_state=2)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size = 0.5, random_state=2)\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape=(59,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=[tf.keras.metrics.MeanSquaredError(), tf.keras.metrics.RootMeanSquaredError(), tf.keras.metrics.MeanAbsoluteError()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Uncomment the lines below to retrain the neural network model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the keras model on the dataset\n",
    "# model.fit(X_train, Y_train, validation_data = (X_val, Y_val), epochs=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training the dataset takes a lot of time, a pre-trained model will be saved to save time.<br>\n",
    "*Uncomment the lines below to retrain the neural network model.*<br>\n",
    "***Do not save unless the model has been retrained***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the Neural Network model into a file\n",
    "# model.save(\"model_NeuralNetwork.keras\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we assess the metrics, such as Mean Square Error (MSE), Root Mean Square Error (RMSE), Mean Absolute Error (MAE). We can see that the Neural Network model has a relatively low MAE and a relatively low RMSE, indicating that it performs well in predicting the target. Note that although the RMSE is in the thousands, its unit is the Philippine Peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Neural Network model from a file\n",
    "loaded = loaded_model = tf.keras.saving.load_model(\"model_NeuralNetwork.keras\")\n",
    "\n",
    "# Evaluating the Neural Network model\n",
    "_, MSE_neural, RMSE_neural, MAE_neural = loaded.evaluate(X_test, Y_test)\n",
    "\n",
    "Y_pred = loaded.predict(X_test)\n",
    "\n",
    "# Show and plot the metrics and the results\n",
    "print(f'MSE: {MSE_neural}')\n",
    "print(f'RMSE: {RMSE_neural}')\n",
    "print(f'MAE: {MAE_neural}')\n",
    "\n",
    "# Plot Predicted vs Target\n",
    "plt.scatter(Y_pred, Y_test)\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Predicted vs Target')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the Metrics of the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, sharex=True, sharey=True, figsize=(15, 20))\n",
    "fig.suptitle('Predicted vs Target')\n",
    "\n",
    "ax1.set_title('Linear Regression')\n",
    "ax1.scatter(y_test_linear, linear_pred)\n",
    "\n",
    "ax2.set_title('Ridge')\n",
    "ax2.scatter(y_test_ridge, ridge_pred)\n",
    "\n",
    "ax3.set_title('Lasso')\n",
    "ax3.scatter(y_test_lasso, lasso_pred)\n",
    "\n",
    "ax4.set_title('Polynomial Regression')\n",
    "ax4.scatter(y_test_poly, poly_pred)\n",
    "\n",
    "ax5.set_title('Neural Network')\n",
    "ax5.scatter(Y_test, Y_pred)\n",
    "\n",
    "ax6.axis('off')\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    ax.label_outer()\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    'Model': ['Linear Regression', 'Ridge', 'Lasso', 'Polynomial Regression', 'Neural Network'],\n",
    "    'MSE': [MSE_linear, MSE_ridge, MSE_lasso, MSE_poly, MSE_neural],\n",
    "    'RMSE': [RMSE_linear, RMSE_ridge, RMSE_lasso, RMSE_poly, RMSE_neural],\n",
    "    'MAE': [MAE_linear, MAE_ridge, MAE_lasso, MAE_poly, MAE_neural],\n",
    "    'R2': [R2_linear, R2_ridge, R2_lasso, R2_poly, \"\"]\n",
    "    }\n",
    "\n",
    "df_metrics = pd.DataFrame(data=metrics)\n",
    "pd.set_option('display.float_format', '{:.10f}'.format)\n",
    "df.style \\\n",
    "  .format(precision=9)\n",
    "df_metrics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the metrics and graph presented, it can be observed that the Neural Network Model exhibits superior performance. However, it should be noted that neural networks, or deep learning models heavily relies on data, necessitating a substantial volume of data to effectively learn the underlying structure and parameters. We believe that the dataset used in this study may not be of sufficient size to establish the reliability of the Neural Network Model. Moreover, neural networks have a tendency to overfit, compromising their predictive capabilities for future instances.\n",
    "\n",
    "In the end, we decided to use the Polynomial Regression Model (2nd Degree). Firstly, this is because Regression Analysis is less of a black box and is easier to communicate.\n",
    "Two important factors we considered when we chose a model are how simple the model is and how interpretable it is. A simpler model means itâ€™s easier to communicate how the model itself works and how to interpret the results of a model. And based on the metrics and the graph, the Polynomial Regression Model outperforms the other models except the Neural Network Model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V. Polynomial Regression Model 2nd Degree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we take into account the interactions between the features. Particualarly, we model these interactions as a second-degree polynomial. Apart from saving computational resources, using this degree yields the best results compared to using higher degrees. This can be explained by factors such as overfitting. We also split the data again for the training and testing of our Polynomial Regression model. It is also important to note that it now uses X_interactions, which account for the feature interactions, instead of the original data X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Regression\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_interactions = poly.fit_transform(X)\n",
    "\n",
    "# Split the data into train and test sets using X_interactions instead of X\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_interactions, y, test_size=0.30, random_state=1)\n",
    "\n",
    "# Fit the model and predict the target\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we assess the metrics, such as RMSE and R2. We can see that our Polynomial Regression model has an R2 close to 1 and a relatively low RMSE, indicating that it performs well in predicting the target. Note that although the RMSE is in the thousands, its unit is the Philippine Peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show and plot the results\n",
    "print(f'RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}')\n",
    "print(f'R2: {r2_score(y_test, y_pred)}')\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Predicted vs Target')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VI. Creating a Web App For the Practical Use of the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Flask, we now make a locally-hosted Web App that predicts the Housing and Water expenditures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict the Housing and Water expenditure given an array of user inputs\n",
    "def model_predict(input_arr):\n",
    "    encoded_input = [input_arr]\n",
    "\n",
    "    # Encode categorical variables\n",
    "    for i in range(len(input_arr)):\n",
    "        if i == 1:\n",
    "            encoded_input[0][i] = convert_region[input_arr[i]]\n",
    "        elif i == 4:\n",
    "            encoded_input[0][i] = convert_house_type[input_arr[i]]\n",
    "        elif i == 8:\n",
    "            encoded_input[0][i] = convert_water_source[input_arr[i]] \n",
    "\n",
    "    # Perform polynomial feature transformation if applicable\n",
    "    input_interactions = poly.fit_transform(encoded_input)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = regressor.predict(input_interactions)\n",
    "\n",
    "    # Return prediction\n",
    "    return float(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "\n",
    "# Change based on Saved location\n",
    "path = r'C:\\Users\\Brylle\\Desktop\\CS 180 Project\\templates'\n",
    "\n",
    "app = Flask(__name__, static_folder = path)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "# Flask route handler\n",
    "@app.route('/calculate', methods=['POST'])\n",
    "def calculate():\n",
    "    features = [key for key in request.form.keys() if key.startswith('input')]\n",
    "    input_arr = [request.form.get(key) for key in features]\n",
    "    conv_input = [float(input_arr[i]) if i not in (1,4,8) else input_arr[i] for i in range(len(input_arr))]\n",
    "    result = 'Php'+str(round(model_predict(conv_input), 2))\n",
    "    return jsonify(result=result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
